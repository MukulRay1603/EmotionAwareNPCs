"""
Task 1: FER Model Evaluation
- Load emotion recognition model
- Test on webcam frames
- Generate accuracy metrics, confusion matrix
- Measure inference time and FPS
- Export to ONNX
"""

import cv2
import numpy as np
import time
import json
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# Try to import DeepFace
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
    print("‚úÖ DeepFace loaded")
except:
    DEEPFACE_AVAILABLE = False
    print("‚ùå DeepFace not available")

# Configuration
OUTPUT_DIR = 'cv/output'
MODELS_DIR = 'cv/models'
EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(MODELS_DIR, exist_ok=True)

class FERModelEvaluator:
    def __init__(self):
        """Initialize FER model evaluator"""
        self.model_name = "DeepFace"
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # Metrics storage
        self.inference_times = []
        self.predictions = []
        self.confidences = []
        self.ground_truth = []  # For manual labeling
        
        print(f"\n{'='*70}")
        print(f"  TASK 1: FER MODEL EVALUATION")
        print(f"{'='*70}\n")
    
    def detect_face(self, frame):
        """Detect largest face in frame"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=5, minSize=(100, 100)
        )
        
        if len(faces) > 0:
            # Return largest face
            return max(faces, key=lambda f: f[2] * f[3])
        return None
    
    def predict_emotion(self, frame):
        """Predict emotion using DeepFace"""
        if not DEEPFACE_AVAILABLE:
            return None, None, None
        
        try:
            start_time = time.time()
            
            result = DeepFace.analyze(
                frame,
                actions=['emotion'],
                enforce_detection=False,
                silent=True
            )
            
            inference_time = time.time() - start_time
            
            if isinstance(result, list):
                result = result[0]
            
            emotions = result['emotion']
            dominant = result['dominant_emotion']
            confidence = emotions[dominant] / 100.0
            
            # Normalize all emotions
            all_emotions = {k: v/100.0 for k, v in emotions.items()}
            
            return dominant, confidence, inference_time, all_emotions
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Prediction error: {e}")
            return None, None, None, None
    
    def collect_test_samples(self, num_samples=30, duration=30):
        """Collect test samples from webcam"""
        print(f"üì∏ Collecting {num_samples} test samples over {duration}s")
        print("   Make different facial expressions!")
        print("   Press SPACE to capture, Q to finish early\n")
        
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("‚ùå Cannot open webcam")
            return False
        
        samples_collected = 0
        start_time = time.time()
        last_capture_time = 0
        capture_interval = duration / num_samples
        
        while samples_collected < num_samples and (time.time() - start_time) < duration:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_time = time.time()
            display_frame = frame.copy()
            
            # Detect face
            face = self.detect_face(frame)
            
            if face is not None:
                x, y, w, h = face
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Auto-capture at intervals OR manual capture with SPACE
            elapsed = current_time - start_time
            time_since_last = current_time - last_capture_time
            
            if (time_since_last >= capture_interval or cv2.waitKey(1) == ord(' ')) and face is not None:
                # Predict emotion
                emotion, confidence, inf_time, all_emotions = self.predict_emotion(frame)
                
                if emotion is not None:
                    samples_collected += 1
                    last_capture_time = current_time
                    
                    # Store metrics
                    self.predictions.append(emotion)
                    self.confidences.append(confidence)
                    self.inference_times.append(inf_time)
                    
                    print(f"  ‚úì Sample {samples_collected:2d}/{num_samples} | "
                          f"{emotion:10s} | Confidence: {confidence:.2f} | "
                          f"Time: {inf_time*1000:.1f}ms")
                    
                    # Visual feedback
                    cv2.putText(display_frame, f"Captured: {emotion}", (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            
            # Display info
            remaining = num_samples - samples_collected
            cv2.putText(display_frame, f"Samples: {samples_collected}/{num_samples}", 
                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(display_frame, f"Time: {elapsed:.1f}s / {duration}s", 
                       (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            cv2.putText(display_frame, "SPACE=Capture  Q=Quit", 
                       (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            
            cv2.imshow('FER Model Evaluation - Sample Collection', display_frame)
            
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        print(f"\n‚úÖ Collected {samples_collected} samples")
        return samples_collected > 0
    
    def generate_metrics(self):
        """Generate performance metrics"""
        print(f"\n{'='*70}")
        print(f"  PERFORMANCE METRICS")
        print(f"{'='*70}\n")
        
        if len(self.inference_times) == 0:
            print("‚ùå No data collected")
            return
        
        # Inference time statistics
        avg_time = np.mean(self.inference_times) * 1000  # ms
        std_time = np.std(self.inference_times) * 1000
        min_time = np.min(self.inference_times) * 1000
        max_time = np.max(self.inference_times) * 1000
        fps = 1 / np.mean(self.inference_times)
        
        print("INFERENCE PERFORMANCE:")
        print(f"  Average Time:     {avg_time:.2f} ms ¬± {std_time:.2f} ms")
        print(f"  Min Time:         {min_time:.2f} ms")
        print(f"  Max Time:         {max_time:.2f} ms")
        print(f"  FPS:              {fps:.2f}")
        print()
        
        # Confidence statistics
        avg_conf = np.mean(self.confidences)
        std_conf = np.std(self.confidences)
        
        print("CONFIDENCE METRICS:")
        print(f"  Average:          {avg_conf:.3f}")
        print(f"  Std Dev:          {std_conf:.3f}")
        print(f"  Min:              {np.min(self.confidences):.3f}")
        print(f"  Max:              {np.max(self.confidences):.3f}")
        print()
        
        # Emotion distribution
        print("EMOTION DISTRIBUTION:")
        emotion_counts = {}
        for emotion in self.predictions:
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        
        for emotion in sorted(emotion_counts.keys()):
            count = emotion_counts[emotion]
            percentage = (count / len(self.predictions)) * 100
            print(f"  {emotion:10s}: {count:3d} ({percentage:5.1f}%)")
        
        print(f"\n{'='*70}\n")
        
        # Create visualizations
        self.create_visualizations(avg_time, std_time, fps, emotion_counts)
        
        # Save metrics to JSON
        self.save_metrics_json(avg_time, std_time, fps, avg_conf, emotion_counts)
    
    def create_visualizations(self, avg_time, std_time, fps, emotion_counts):
        """Create visualization plots"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Inference Time Distribution
        ax = axes[0, 0]
        times_ms = [t * 1000 for t in self.inference_times]
        ax.hist(times_ms, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
        ax.axvline(avg_time, color='red', linestyle='--', linewidth=2, label=f'Mean: {avg_time:.1f}ms')
        ax.set_xlabel('Inference Time (ms)', fontweight='bold')
        ax.set_ylabel('Frequency', fontweight='bold')
        ax.set_title('Inference Time Distribution', fontweight='bold', fontsize=12)
        ax.legend()
        ax.grid(alpha=0.3)
        
        # 2. Confidence Distribution
        ax = axes[0, 1]
        ax.hist(self.confidences, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)
        ax.axvline(np.mean(self.confidences), color='red', linestyle='--', 
                   linewidth=2, label=f'Mean: {np.mean(self.confidences):.2f}')
        ax.set_xlabel('Confidence Score', fontweight='bold')
        ax.set_ylabel('Frequency', fontweight='bold')
        ax.set_title('Prediction Confidence Distribution', fontweight='bold', fontsize=12)
        ax.legend()
        ax.grid(alpha=0.3)
        
        # 3. Emotion Distribution Pie Chart
        ax = axes[1, 0]
        colors = plt.cm.Set3(range(len(emotion_counts)))
        ax.pie(emotion_counts.values(), labels=emotion_counts.keys(), autopct='%1.1f%%',
               colors=colors, startangle=90)
        ax.set_title('Emotion Distribution', fontweight='bold', fontsize=12)
        
        # 4. Performance Summary
        ax = axes[1, 1]
        ax.axis('off')
        summary_text = f"""
PERFORMANCE SUMMARY

Inference Speed:
  ‚Ä¢ Average: {avg_time:.2f} ms
  ‚Ä¢ Std Dev: {std_time:.2f} ms
  ‚Ä¢ FPS: {fps:.2f}

Confidence:
  ‚Ä¢ Average: {np.mean(self.confidences):.3f}
  ‚Ä¢ Std Dev: {np.std(self.confidences):.3f}

Total Samples: {len(self.predictions)}
Model: {self.model_name}
        """
        ax.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',
                verticalalignment='center')
        
        plt.tight_layout()
        output_path = os.path.join(OUTPUT_DIR, 'task1_fer_evaluation.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"‚úÖ Visualizations saved: {output_path}")
        plt.close()
    
    def save_metrics_json(self, avg_time, std_time, fps, avg_conf, emotion_counts):
        """Save metrics to JSON file"""
        metrics = {
            "task": "Task 1: FER Model Evaluation",
            "timestamp": datetime.now().isoformat(),
            "model": self.model_name,
            "inference_performance": {
                "average_time_ms": round(avg_time, 2),
                "std_time_ms": round(std_time, 2),
                "min_time_ms": round(np.min(self.inference_times) * 1000, 2),
                "max_time_ms": round(np.max(self.inference_times) * 1000, 2),
                "fps": round(fps, 2)
            },
            "confidence_metrics": {
                "average": round(avg_conf, 3),
                "std_dev": round(np.std(self.confidences), 3),
                "min": round(np.min(self.confidences), 3),
                "max": round(np.max(self.confidences), 3)
            },
            "emotion_distribution": emotion_counts,
            "total_samples": len(self.predictions),
            "all_predictions": self.predictions,
            "all_confidences": [round(c, 3) for c in self.confidences],
            "all_inference_times_ms": [round(t * 1000, 2) for t in self.inference_times]
        }
        
        json_path = os.path.join(OUTPUT_DIR, 'task1_metrics.json')
        with open(json_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print(f"‚úÖ Metrics JSON saved: {json_path}")
    
    def export_to_onnx(self):
        """Export model to ONNX format"""
        print(f"\n{'='*70}")
        print(f"  ONNX EXPORT")
        print(f"{'='*70}\n")
        
        print("‚ö†Ô∏è  Note: DeepFace uses multiple backends (VGG-Face, etc.)")
        print("   Direct ONNX export requires extracting the underlying model")
        print("   For now, we'll document the process:\n")
        
        export_info = {
            "status": "documented",
            "model": "DeepFace (VGG-Face backend)",
            "notes": [
                "DeepFace is a wrapper around multiple models",
                "To export to ONNX, need to extract the base model (VGG-Face, etc.)",
                "Alternative: Use tf2onnx for TensorFlow models",
                "Command: python -m tf2onnx.convert --saved-model <path> --output model.onnx"
            ],
            "recommendation": "Use DeepFace as-is for Phase 1, optimize in Phase 2"
        }
        
        onnx_info_path = os.path.join(OUTPUT_DIR, 'task1_onnx_export_info.json')
        with open(onnx_info_path, 'w') as f:
            json.dump(export_info, f, indent=2)
        
        print(f"‚úÖ ONNX export info saved: {onnx_info_path}")
        print(f"   For production: Consider using ONNX Runtime for faster inference\n")
    
    def run(self):
        """Run full evaluation pipeline"""
        if not DEEPFACE_AVAILABLE:
            print("‚ùå DeepFace not available. Cannot proceed.")
            return
        
        # Step 1: Collect samples
        success = self.collect_test_samples(num_samples=30, duration=60)
        
        if not success:
            print("‚ùå Sample collection failed")
            return
        
        # Step 2: Generate metrics
        self.generate_metrics()
        
        # Step 3: ONNX export info
        self.export_to_onnx()
        
        print(f"\n{'='*70}")
        print(f"  ‚úÖ TASK 1 COMPLETE")
        print(f"{'='*70}\n")
        print("Output files:")
        print(f"  ‚Ä¢ {OUTPUT_DIR}/task1_fer_evaluation.png")
        print(f"  ‚Ä¢ {OUTPUT_DIR}/task1_metrics.json")
        print(f"  ‚Ä¢ {OUTPUT_DIR}/task1_onnx_export_info.json")
        print()

def main():
    evaluator = FERModelEvaluator()
    evaluator.run()

if __name__ == "__main__":
    main()